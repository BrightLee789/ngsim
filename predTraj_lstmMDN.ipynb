{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dongwan123\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.io as sio\n",
    "%matplotlib inline\n",
    "\n",
    "# GPU settings\n",
    "config = tf.ConfigProto()\n",
    "config.allow_soft_placement = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "could not read bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3ab33709aa29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./train_data/trainData_i80_0400-0415.mat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mf_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'f_data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprevTraj_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'prevTraj_data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpostTraj_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'postTraj_data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\io\\matlab\\mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'variable_names'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[0mMR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_opened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m     \u001b[0mmatfile_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmdict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mmdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatfile_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\io\\matlab\\mio5.py\u001b[0m in \u001b[0;36mget_variables\u001b[1;34m(self, variable_names)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_var_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhdr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mMatReadError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 warnings.warn(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\io\\matlab\\mio5.py\u001b[0m in \u001b[0;36mread_var_array\u001b[1;34m(self, header, process)\u001b[0m\n\u001b[0;32m    250\u001b[0m            \u001b[0;31m`\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         '''\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matrix_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_from_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_real_complex\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_numeric\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_element\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mstreams.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.streams.GenericStream.read_string\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mstreams.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.streams.GenericStream.read_into\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: could not read bytes"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "mat = sio.loadmat('./train_data/trainData_i80_0400-0415.mat')\n",
    "f_data = np.array(mat['f_data'])\n",
    "prevTraj_data = np.array(mat['prevTraj_data'])\n",
    "postTraj_data = np.array(mat['postTraj_data'])\n",
    "\n",
    "# Set train-data\n",
    "f_trainData = f_data\n",
    "prevTraj_trainData = prevTraj_data\n",
    "postTraj_trainData = postTraj_data\n",
    "\n",
    "num_traindata = f_trainData.shape[0] # number of train-data\n",
    "\n",
    "f_mean_trainData = np.mean(f_trainData,axis=0)\n",
    "prevTraj_mean_trainData = np.mean(prevTraj_trainData,axis=0)\n",
    "postTraj_mean_trainData = np.mean(postTraj_trainData,axis=0)\n",
    "\n",
    "f_trainData = f_trainData - np.tile(f_mean_trainData,(num_traindata,1))\n",
    "prevTraj_trainData = prevTraj_trainData - np.tile(prevTraj_mean_trainData,(num_traindata,1))\n",
    "postTraj_trainData = postTraj_trainData - np.tile(postTraj_mean_trainData,(num_traindata,1))\n",
    "\n",
    "f_absmax = np.max(np.abs(f_trainData),axis=0)\n",
    "f_trainData = f_trainData/np.tile(f_absmax,(num_traindata,1))\n",
    "prevTraj_absmax = np.max(np.abs(prevTraj_trainData),axis=0)\n",
    "prevTraj_trainData = prevTraj_trainData/np.tile(prevTraj_absmax,(num_traindata,1))\n",
    "postTraj_absmax = np.max(np.abs(postTraj_trainData),axis=0)\n",
    "postTraj_trainData = postTraj_trainData/np.tile(postTraj_absmax,(num_traindata,1))\n",
    "\n",
    "dim_x = 3               # dimension of state\n",
    "Hx_prev = 15            # timesteps of input (horizon) : less than 6\n",
    "Hx_post = 15            # timesteps of post (horizon) : less than 10\n",
    "dim_t = dim_x*Hx_post   # dimension of trajectory\n",
    "dim_f = f_trainData.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_lstm = dim_x*3\n",
    "n_components_mdn = 12\n",
    "n_outputs_mdn = (2*dim_t + 1)*n_components_mdn\n",
    "lam_mdn = 0.01\n",
    "\n",
    "#hidden_size_mdn = n_outputs_mdn*2\n",
    "hidden_size_mdn = [(n_outputs_mdn)*4, (n_outputs_mdn)*4]\n",
    "\n",
    "sigma_max = 10 # max value of variance\n",
    "\n",
    "# Set epsilon value\n",
    "epsilon_init = 1e-5\n",
    "decayRate_epsilon =0.95\n",
    "decaySteps_epsilon = 200\n",
    "learning_epsilon = epsilon_init\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, Hx_prev, dim_x])\n",
    "Y = tf.placeholder(\"float\", [None, dim_t])\n",
    "F = tf.placeholder(\"float\", [None, dim_f])\n",
    "EPS = tf.placeholder(\"float\", None)\n",
    "\n",
    "# Save params\n",
    "filename2save0 = \"./tf_saved_model/params_predTraj_lstmMDN_ti%dto%d\" % (Hx_prev,Hx_post)\n",
    "sio.savemat(filename2save0, {'f_mean_trainData':f_mean_trainData,'prevTraj_mean_trainData':prevTraj_mean_trainData,\n",
    "                             'postTraj_mean_trainData':postTraj_mean_trainData,'f_absmax':f_absmax,'prevTraj_absmax':prevTraj_absmax,\n",
    "                             'postTraj_absmax':postTraj_absmax,'n_hidden_lstm':n_hidden_lstm,'n_components_mdn':n_components_mdn,\n",
    "                             'sigma_max':sigma_max})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTMwMDN(x_, f_):\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    \n",
    "    x_ = tf.unstack(x_, Hx_prev, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(n_hidden_lstm)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    rnn_outputs, rnn_states = rnn.static_rnn(lstm_cell, x_, dtype=tf.float32)\n",
    "    \n",
    "    rnn_encoded = rnn_outputs[-1]\n",
    "    \n",
    "    x_concat = tf.concat([rnn_encoded, f_], 1)\n",
    "\n",
    "    output_mdn_1 = tf.layers.dense(x_concat, \n",
    "                           hidden_size_mdn[0], \n",
    "                           activation = tf.nn.relu, \n",
    "                           kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                           bias_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                           kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lam_mdn)\n",
    "                          )\n",
    "    \n",
    "    output_mdn_2 = tf.layers.dense(output_mdn_1, \n",
    "                           hidden_size_mdn[1], \n",
    "                           activation = tf.nn.relu,\n",
    "                           kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                           bias_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                           kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lam_mdn)\n",
    "                          )\n",
    "    output_mdn_out = tf.layers.dense(output_mdn_2, \n",
    "                           n_outputs_mdn,\n",
    "                           kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                           bias_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                           kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lam_mdn)\n",
    "                          )\n",
    "    \n",
    "    indexes_split = [dim_t*n_components_mdn, dim_t*n_components_mdn, n_components_mdn]\n",
    "    means_mdn_out, sigma_mdn_act, fracs_mdn_act = tf.split(output_mdn_out, indexes_split, axis=1)\n",
    "\n",
    "    sigma_mdn_out = sigma_max*tf.nn.sigmoid(sigma_mdn_act)\n",
    "\n",
    "    fracs_mdn_out_max = tf.reduce_max(fracs_mdn_act,axis=1)\n",
    "    fracs_mdn_out_max_ext_ = tf.expand_dims(fracs_mdn_out_max, 1)\n",
    "    fracs_mdn_out_max_ext = tf.tile(fracs_mdn_out_max_ext_,[1, n_components_mdn])\n",
    "    fracs_mdn_out = fracs_mdn_act - fracs_mdn_out_max_ext\n",
    "    fracs_mdn_out = tf.nn.softmax(fracs_mdn_out)\n",
    "    \n",
    "    return means_mdn_out, sigma_mdn_out, fracs_mdn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp_trick(x_):\n",
    "    xmax_ = tf.reduce_max(x_,axis = 1)\n",
    "    xmax_ext__ = tf.expand_dims(xmax_, 1)\n",
    "    xmax_ext_ = tf.tile(xmax_ext__,[1, n_components_mdn])\n",
    "    diffx_ = x_ - xmax_ext_\n",
    "    x_logsumexp_ = tf.reduce_logsumexp(diffx_,1)\n",
    "    x_logsumexp = xmax_ + x_logsumexp_\n",
    "    \n",
    "    return x_logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN graph + MDN\n",
    "means_mdn_, sigma_mdn_, fracs_mdn_ = LSTMwMDN(X, F)\n",
    "means_mdn = tf.reshape(means_mdn_,[-1,dim_t,n_components_mdn]) # (num of data, dim_out*horizon_out, num of components)\n",
    "sigma_mdn = tf.reshape(sigma_mdn_,[-1,dim_t,n_components_mdn]) # (num of data, dim_out*horizon_out, num of components)\n",
    "fracs_mdn = tf.reshape(fracs_mdn_,[-1,n_components_mdn]) # (num of data, num of components)\n",
    "\n",
    "# Define loss and optimizer\n",
    "Y_ext_ = tf.expand_dims(Y, 2)\n",
    "Y_ext = tf.tile(Y_ext_,[1,1,n_components_mdn]) # (num of data, dim_out*horizon_out, num of components)\n",
    "diff = Y_ext - means_mdn\n",
    "\n",
    "squared_diff = tf.square(diff)\n",
    "scaled_squared_diff = tf.div(squared_diff,(sigma_mdn+EPS))\n",
    "scaled_dist = tf.reduce_sum(scaled_squared_diff,1) # (num of data, num of components)\n",
    "\n",
    "logsum_sigma_mdn = tf.reduce_sum(tf.log(sigma_mdn+EPS),1) # (num of data, num of components)\n",
    "\n",
    "loss_exponent = tf.log(fracs_mdn+EPS) - 0.5*dim_t*tf.log(np.pi*2) - 0.5*logsum_sigma_mdn - 0.5*scaled_dist\n",
    "# loss_op = tf.reduce_mean(-tf.reduce_logsumexp(loss_exponent,axis=1))\n",
    "loss_op = -tf.reduce_mean(logsumexp_trick(loss_exponent))\n",
    "\n",
    "learning_rate = 0.001\n",
    "# train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss_op)\n",
    "train_op = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=0.8).minimize(loss_op)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Create a saver object which will save all the variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Export meta graph\n",
    "tf.add_to_collection('X', X)\n",
    "tf.add_to_collection('Y', Y)\n",
    "tf.add_to_collection('F', F)\n",
    "tf.add_to_collection('means_mdn', means_mdn)\n",
    "tf.add_to_collection('sigma_mdn', sigma_mdn)\n",
    "tf.add_to_collection('fracs_mdn', fracs_mdn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, epsilon 0.000010, Minibatch Loss= -42.3842751, sigma= 0.000717/0.005304/0.029754\n",
      "epoch 50, epsilon 0.000010, Minibatch Loss= -155.5826875, sigma= 0.000000/0.000411/0.008790\n",
      "epoch 100, epsilon 0.000010, Minibatch Loss= -157.0464780, sigma= 0.000000/0.000300/0.009226\n",
      "epoch 150, epsilon 0.000010, Minibatch Loss= -159.8415329, sigma= 0.000000/0.000622/0.023783\n",
      "epoch 200, epsilon 0.000010, Minibatch Loss= -159.7974338, sigma= 0.000000/0.000713/0.035433\n",
      "epoch 250, epsilon 0.000010, Minibatch Loss= -158.5976043, sigma= 0.000000/0.000444/0.013901\n",
      "epoch 300, epsilon 0.000010, Minibatch Loss= -158.8930314, sigma= 0.000000/0.000419/0.053860\n",
      "epoch 350, epsilon 0.000010, Minibatch Loss= -158.5232968, sigma= 0.000000/0.001003/0.085903\n",
      "epoch 400, epsilon 0.000009, Minibatch Loss= -161.5180862, sigma= 0.000000/0.000878/0.037936\n",
      "epoch 450, epsilon 0.000009, Minibatch Loss= -159.7439754, sigma= 0.000000/0.000537/0.062314\n",
      "epoch 500, epsilon 0.000009, Minibatch Loss= -157.9385229, sigma= 0.000000/0.001261/0.135410\n",
      "epoch 550, epsilon 0.000009, Minibatch Loss= -163.0460245, sigma= 0.000000/0.000428/0.023615\n",
      "epoch 600, epsilon 0.000009, Minibatch Loss= -148.4081847, sigma= 0.000000/0.002069/0.088368\n",
      "epoch 650, epsilon 0.000009, Minibatch Loss= -160.0496247, sigma= 0.000000/0.001846/0.152572\n",
      "epoch 700, epsilon 0.000009, Minibatch Loss= -153.1792538, sigma= 0.000000/0.001533/0.067298\n",
      "epoch 750, epsilon 0.000009, Minibatch Loss= -149.4600848, sigma= 0.000000/0.000867/0.050586\n",
      "epoch 800, epsilon 0.000008, Minibatch Loss= -158.0661188, sigma= 0.000000/0.000798/0.044887\n",
      "epoch 850, epsilon 0.000008, Minibatch Loss= -160.1343628, sigma= 0.000000/0.001504/0.134759\n",
      "epoch 900, epsilon 0.000008, Minibatch Loss= -158.4707912, sigma= 0.000000/0.000398/0.088992\n",
      "epoch 950, epsilon 0.000008, Minibatch Loss= -156.2202232, sigma= 0.000000/0.001004/0.306435\n",
      "epoch 1000, epsilon 0.000008, Minibatch Loss= -157.5734465, sigma= 0.000000/0.004374/2.176343\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (20, 1, 7) for Tensor 'Placeholder_2:0', which has shape '(?, 7)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0b3988a11e16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_trainData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mmeans_mdn_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans_mdn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf_test\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0msigma_mdn_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma_mdn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf_test\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mfracs_mdn_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfracs_mdn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf_test\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1114\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1116\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1117\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (20, 1, 7) for Tensor 'Placeholder_2:0', which has shape '(?, 7)'"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "training_epochs = 1000\n",
    "batch_size = 256\n",
    "num_batches = np.ceil(num_traindata/batch_size)\n",
    "num_batches = num_batches.astype(np.int32)\n",
    "display_epochs = 50\n",
    "save_epochs = 200\n",
    "\n",
    "h_x_train = np.array(range(0,dim_x*Hx_prev), dtype = np.int32)\n",
    "h_y_train = np.array(range(0,dim_x*Hx_post), dtype = np.int32)\n",
    "\n",
    "# Suffle indexes\n",
    "idx_array = np.arange(num_traindata)\n",
    "np.random.shuffle(idx_array)\n",
    "\n",
    "with tf.Session(config = config) as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "    loss_epochs = np.zeros(training_epochs)\n",
    "    # Train phase\n",
    "    for epoch in range(1, training_epochs+1):\n",
    "        \n",
    "        # Suffle indexes\n",
    "        np.random.shuffle(idx_array)\n",
    "        \n",
    "        # Set epsilon value\n",
    "        if (epoch % decaySteps_epsilon) == 0 and epoch > 1:\n",
    "            learning_epsilon = learning_epsilon*decayRate_epsilon\n",
    "        \n",
    "        cnt_data = 0\n",
    "        loss_array = np.zeros(num_batches)\n",
    "        for nidx_batch in range(0,num_batches):\n",
    "            # Get batch data\n",
    "            if((cnt_data + batch_size) > num_traindata):\n",
    "                idx_batch = idx_array[range((cnt_data),(num_traindata))]\n",
    "            else:\n",
    "                idx_batch = idx_array[range((cnt_data),(cnt_data + batch_size))]\n",
    "                \n",
    "            x_batch = prevTraj_trainData[idx_batch,:]\n",
    "            x_batch = x_batch[:,h_x_train]\n",
    "            x_batch = x_batch.reshape(len(idx_batch),Hx_prev,dim_x)  # convert to (batch_size, timesteps, dim_x)\n",
    "            \n",
    "            y_batch = postTraj_trainData[idx_batch,:]\n",
    "            y_batch = y_batch[:,h_y_train]\n",
    "            \n",
    "            f_batch = f_trainData[idx_batch,:]\n",
    "            \n",
    "            cnt_data = cnt_data + len(idx_batch)\n",
    "            \n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={X: x_batch, Y: y_batch, F: f_batch, EPS: learning_epsilon})\n",
    "            \n",
    "            # Compute batch loss\n",
    "            loss_val = sess.run(loss_op, feed_dict={X: x_batch, Y: y_batch, F: f_batch, EPS: learning_epsilon})\n",
    "            loss_array[nidx_batch] = loss_val\n",
    "        \n",
    "        loss_epochs[epoch-1] = np.mean(loss_array)\n",
    "        \n",
    "        if epoch % display_epochs == 0 or epoch == 1:\n",
    "            # Check sigma\n",
    "            val_sigma_mdn = sess.run(sigma_mdn, feed_dict={X: x_batch, F: f_batch})\n",
    "            minval_sigma_mdn = np.min(val_sigma_mdn)\n",
    "            meanval_sigma_mdn = np.mean(val_sigma_mdn)\n",
    "            maxval_sigma_mdn = np.max(val_sigma_mdn)\n",
    "            \n",
    "            print(\"epoch \" + str(epoch) + \", epsilon {:.6f}\".format(learning_epsilon) +\n",
    "                  \", Minibatch Loss= \" + \"{:.7f}\".format(np.mean(loss_array)) + \n",
    "                  \", sigma= \" + \"{:.6f}/{:.6f}/{:.6f}\".format(minval_sigma_mdn,meanval_sigma_mdn,maxval_sigma_mdn))\n",
    "            \n",
    "        # Save\n",
    "        if epoch % save_epochs == 0 or epoch == 1:\n",
    "            # Now, save the graph\n",
    "            filename2save1 = \"./tf_saved_model/predTraj_lstmMDN_ti%dto%d\" % (Hx_prev,Hx_post)\n",
    "            saver.save(sess,filename2save1,global_step=100)\n",
    "            \n",
    "        \n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    # Test phase\n",
    "    idx_test = np.random.randint(num_traindata,size=(20,1))\n",
    "    x_test = prevTraj_trainData[idx_test,h_x_train]\n",
    "    x_test = np.reshape(x_test,(idx_test.shape[0],Hx_prev,dim_x))\n",
    "    \n",
    "    y_test = postTraj_trainData[idx_test,:]\n",
    "    y_test = postTraj_trainData[:,h_y_train]\n",
    "    \n",
    "    f_test = f_trainData[idx_test,:]\n",
    "    \n",
    "    means_mdn_test = sess.run(means_mdn, feed_dict={X: x_test, F: f_test})\n",
    "    sigma_mdn_test = sess.run(sigma_mdn, feed_dict={X: x_test, F: f_test})\n",
    "    fracs_mdn_test = sess.run(fracs_mdn, feed_dict={X: x_test, F: f_test})\n",
    "    \n",
    "    # Now, save test results\n",
    "    filename2save2 = \"./tf_saved_model/results_predTraj_lstmMDN_ti%dto%d\" % (Hx_prev,Hx_post)\n",
    "    sio.savemat(filename2save2, {'means_mdn_test':means_mdn_test,'sigma_mdn_test':sigma_mdn_test,'fracs_mdn_test':fracs_mdn_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-17a04c274751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "x_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
